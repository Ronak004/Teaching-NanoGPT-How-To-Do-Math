````md
# Teaching NanoGPT to do Math (DPO Fine-Tuning)

This project fine-tunes a lightweight **NanoGPT** model to solve basic arithmetic and algebra-style equations using **Direct Preference Optimization (DPO)**. The model learns from **positive vs negative answer pairs** to improve its ability to output correct numeric answers for math prompts.

---

## âœ¨ Key Features

- Fine-tuned NanoGPT to answer:
  - Addition: `a+b=?`
  - Subtraction: `a-b=?`
  - Multiplication: `a*b=?`
  - Division (exact integer division): `a/b=?`
  - Linear equations:
    - `x*b=c, x=?`
    - `b-x=c, x=?`
    - `x+b=c, x=?`
- Preference dataset generation with:
  - **Correct positive answers** (with explanation format)
  - **Incorrect / weak negative answers** (e.g., â€œSorry, I donâ€™t knowâ€ or near-miss wrong numbers)
- Trained using **DPO loss** on GPU for fast convergence
- Added evaluation scripts to test model outputs on custom prompts

---

## ğŸ“‚ Repository Structure

```text
NanoGPT-Math/
â”‚
â”œâ”€â”€ dpo/
â”‚   â”œâ”€â”€ dpo.ipynb                  # Main notebook for DPO training + evaluation
â”‚   â”œâ”€â”€ pos_neg_pairs.json         # Preference dataset (positive/negative pairs)
â”‚   â”œâ”€â”€ dpo_epoch*.pt              # Saved checkpoints
â”‚
â”œâ”€â”€ sft/
â”‚   â”œâ”€â”€ gpt.pt                     # Base pretrained checkpoint (starting point)
â”‚   â”œâ”€â”€ meta.pkl                   # Tokenizer metadata (stoi/itos)
â”‚
â”œâ”€â”€ model.py                       # NanoGPT model implementation
â”œâ”€â”€ generate_pairs.py              # Dataset generation script (math pairs)
â””â”€â”€ README.md
````

---

## âœ… Dataset Format (DPO Training)

Each training sample is stored as a JSON object:

```json
{
  "positive": "72/4=? The answer is 18 because 72/4 equals 18.",
  "negative": "72/4=? Sorry, I do not know!",
  "Q": "72/4=?",
  "A": "18",
  "type": "a_over_b_q"
}
```

âœ… Only `positive` and `negative` are used during DPO training.
ğŸ“ Fields like `Q`, `A`, and `type` are included for evaluation/debugging.

---

## ğŸ§  Training Method (DPO)

We optimize the model to prefer **positive outputs** over **negative outputs** using the DPO loss:

[
\mathcal{L}*{DPO} = -\log \sigma\left(\frac{\log p*\theta(y^+|x) - \log p_\theta(y^-|x)}{\beta}\right)
]

Where:

* (y^+) is the preferred (correct) completion
* (y^-) is the non-preferred (incorrect) completion
* (\beta) controls preference sharpness

---

## ğŸš€ How to Run

### 1) Install dependencies

```bash
pip install torch numpy tqdm matplotlib
```

### 2) Generate dataset (100k pairs)

```bash
python generate_pairs.py
```

### 3) Train using DPO

Open the notebook:

```bash
dpo/dpo.ipynb
```

Run training to produce checkpoints such as:

```text
dpo/dpo_epoch1.pt ... dpo/dpo_epoch10.pt
```

### 4) Evaluate

Example evaluation prompts:

```python
test_set = [
  "17+19=?",
  "3*17=?",
  "72/4=?",
  "72-x=34, x=?",
  "x*11=44, x=?"
]
```

---

## ğŸ“Š Example Outputs

**Input**

```text
Q: 72/4=?
```

**Model**

```text
A: 18
```

**Input**

```text
Q: x*11=44, x=?
```

**Model**

```text
A: 4
```

---

## âš™ï¸ Notes / Debugging

* During testing, prompts should match training formatting:

  * Use the cue: `"The answer is "`
* For equation prompts, spacing can matter:

  * `72-x=34, x=?` can behave differently from `72-x=34,x=?` if the training data only contains one format.
* If CUDA errors occur (example: `device-side assert triggered`), restart the runtime and load checkpoints on CPU first before moving to GPU.

---

## ğŸ‘¨â€ğŸ’» Contributors

Completed as part of **SC3000 (Machine Learning)** assignment work, focused on:

* Preference dataset creation (positive/negative pairs)
* DPO training loop implementation
* Debugging dataset/prompt formatting issues
* Model evaluation and checkpointing

---

## ğŸ“Œ Acknowledgements

* NanoGPT implementation inspired by Andrej Karpathyâ€™s NanoGPT
* DPO based on preference optimization techniques commonly used in LLM alignment research

```

```

